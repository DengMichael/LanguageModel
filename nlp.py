# -*- coding: utf-8 -*-
"""NLP

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dM6CqIjLMW57v5Rv8VYAlZA0q98CpTCL

# 2a - define the datasets, and run the model you want to train

## Define TFIDF Datasets
"""

import pandas as pd
import torch
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import os
import pickle
from spacy.lang.en.stop_words import STOP_WORDS
from scipy.spatial.distance import cosine
from torch.utils.data import Dataset
from sklearn.preprocessing import LabelEncoder
import random
pd.set_option('display.max_columns', None)


def convert_into_related(label):
    if label == "agree" or label == "disagree" or label == "discuss":
        label = "related"
    return label


class TrainingTFIDFDataset(Dataset):
    def __init__(self):
        related_encoder = LabelEncoder()
        related_encoder.fit(["related", "unrelated"])
        stance_encoder = LabelEncoder()
        stance_encoder.fit(["agree", "disagree", "discuss", "unrelated"]) # we add unrelated for this one too

        # Load datasets
        train_bodies = pd.read_csv("train_bodies.csv")

        train_stances = pd.read_csv("train_stances.csv")

        test_bodies = pd.read_csv("competition_test_bodies.csv")

        test_stances = pd.read_csv("competition_test_stances.csv")

        # Merge the two into one
        train_merge = pd.merge(train_bodies, train_stances,
                            on="Body ID", how="left")
        test_merge = pd.merge(test_bodies, test_stances,
                            on="Body ID", how="left")

        self.train_bodies = []
        self.train_headlines = []
        self.test_bodies = []
        self.test_headlines = []
        self.train_features = []
        self.train_stances = []
        self.train_relateds = []

        for row in train_merge.iterrows():
            body = row[1]["articleBody"]
            headline = row[1]["Headline"]
            if body not in self.train_bodies:
                self.train_bodies.append(body)
            if headline not in self.train_headlines:
                self.train_headlines.append(headline)

        for row in test_merge.iterrows():
            body = row[1]["articleBody"]
            headline = row[1]["Headline"]
            if body not in self.test_bodies:
                self.test_bodies.append(body)
            if headline not in self.test_headlines:
                self.test_headlines.append(headline)

        # Run vectorizer on them
        if os.path.isfile("pickled_tfidf_vectorizer"):
            self.tfidf_vectorizer = pickle.load(open("pickled_tfidf_vectorizer", "rb"))
        else:
            self.tfidf_vectorizer = TfidfVectorizer(
                lowercase=True, stop_words=STOP_WORDS, max_features=512)
            self.tfidf_vectorizer = self.tfidf_vectorizer.fit(self.train_headlines + self.train_bodies + self.test_headlines + self.test_bodies)
            pickle.dump(self.tfidf_vectorizer, open("pickled_tfidf_vectorizer", "wb"))

        if os.path.isfile("pickled_count_vectorizer"):
            self.count_vectorizer = pickle.load(open("pickled_count_vectorizer", "rb"))
        else:
            self.count_vectorizer = CountVectorizer(
                lowercase=True, stop_words=STOP_WORDS, max_features=512)
            self.count_vectorizer = self.count_vectorizer.fit(self.train_headlines + self.train_bodies + self.test_headlines + self.test_bodies)
            pickle.dump(self.count_vectorizer, open("pickled_count_vectorizer", "wb"))
        
        if os.path.isfile("pickled_training_tfidf_features") and os.path.isfile("pickled_training_tfidf_stances") and os.path.isfile("pickled_training_tfidf_stances"):
            self.train_features = torch.load("pickled_training_tfidf_features")
            self.train_stances = torch.load("pickled_training_tfidf_stances")
            self.train_relateds = torch.load("pickled_training_tfidf_relateds")
        else:
            for row in train_merge.sample(1000).iterrows():
                body = row[1]["articleBody"]
                headline = row[1]["Headline"]
                stance = row[1]["Stance"]

                train_bodies_vec = self.tfidf_vectorizer.transform([body]).toarray()
                train_stances_vec = self.tfidf_vectorizer.transform([headline]).toarray()

                # Compute cosine similarity
                cosine_vec = cosine_similarity(train_bodies_vec, train_stances_vec)

                feature_vec = np.squeeze(
                    np.c_[train_bodies_vec, cosine_vec, train_stances_vec])
                self.train_features.append(feature_vec)
                self.train_stances.append(stance_encoder.transform([stance]).item())
                self.train_relateds.append(related_encoder.transform(
                    [convert_into_related(stance)]).item())
            torch.save(self.train_features, "pickled_training_tfidf_features")
            torch.save(self.train_stances, "pickled_training_tfidf_stances")
            torch.save(self.train_relateds, "pickled_training_tfidf_relateds")
        self.class_weights_2b = class_weight.compute_class_weight('balanced',
                                                np.unique(self.train_stances),
                                                self.train_stances)
        self.class_weights_2a = class_weight.compute_class_weight('balanced',
                                                np.unique(self.train_relateds),
                                                self.train_relateds)


    def __len__(self):
        return len(self.train_features)

    def get_vocab_size(self):
        return self.count_vectorizer.get_feature_names()

    def get_feature_size(self):
        return len(self.train_features[0])

    def __getitem__(self, i):
        return self.train_features[i], self.train_stances[i], self.train_relateds[i]


class Training2bTFIDFDataset(Dataset):
    def __init__(self):
        stance_encoder = LabelEncoder()
        stance_encoder.fit(["agree", "disagree", "discuss"])

        # Load datasets
        train_bodies = pd.read_csv("train_bodies.csv")

        train_stances = pd.read_csv("train_stances.csv")

        test_bodies = pd.read_csv("competition_test_bodies.csv")

        test_stances = pd.read_csv("competition_test_stances.csv")

        # Merge the two into one
        train_merge = pd.merge(train_bodies, train_stances,
                            on="Body ID", how="left")
        test_merge = pd.merge(test_bodies, test_stances,
                            on="Body ID", how="left")

        self.train_bodies = []
        self.train_headlines = []
        self.test_bodies = []
        self.test_headlines = []
        self.train_features = []
        self.train_stances = []
        self.train_relateds = []

        for row in train_merge.iterrows():
            body = row[1]["articleBody"]
            headline = row[1]["Headline"]
            if body not in self.train_bodies:
                self.train_bodies.append(body)
            if headline not in self.train_headlines:
                self.train_headlines.append(headline)

        for row in test_merge.iterrows():
            body = row[1]["articleBody"]
            headline = row[1]["Headline"]
            if body not in self.test_bodies:
                self.test_bodies.append(body)
            if headline not in self.test_headlines:
                self.test_headlines.append(headline)

        # Run vectorizer on them
        if os.path.isfile("pickled_tfidf_vectorizer"):
            self.tfidf_vectorizer = pickle.load(open("pickled_tfidf_vectorizer", "rb"))
        else:
            self.tfidf_vectorizer = TfidfVectorizer(
                lowercase=True, stop_words=STOP_WORDS, max_features=512)
            self.tfidf_vectorizer = self.tfidf_vectorizer.fit(self.train_headlines + self.train_bodies + self.test_headlines + self.test_bodies)
            pickle.dump(self.tfidf_vectorizer, open("pickled_tfidf_vectorizer", "wb"))

        if os.path.isfile("pickled_count_vectorizer"):
            self.count_vectorizer = pickle.load(open("pickled_count_vectorizer", "rb"))
        else:
            self.count_vectorizer = CountVectorizer(
                lowercase=True, stop_words=STOP_WORDS, max_features=512)
            self.count_vectorizer = self.count_vectorizer.fit(self.train_headlines + self.train_bodies + self.test_headlines + self.test_bodies)
            pickle.dump(self.count_vectorizer, open("pickled_count_vectorizer", "wb"))
        
        if os.path.isfile("pickled_training_2b_tfidf_features") and os.path.isfile("pickled_training_2b_tfidf_stances"):
            self.train_features = torch.load("pickled_training_2b_tfidf_features")
            self.train_stances = torch.load("pickled_training_2b_tfidf_stances")
        else:
            for row in train_merge.sample(1000).iterrows():
                body = row[1]["articleBody"]
                headline = row[1]["Headline"]
                stance = row[1]["Stance"]
                if stance != "unrelated":
                    train_bodies_vec = self.tfidf_vectorizer.transform([body]).toarray()
                    train_stances_vec = self.tfidf_vectorizer.transform([headline]).toarray()

                    # Compute cosine similarity
                    cosine_vec = cosine_similarity(train_bodies_vec, train_stances_vec)

                    feature_vec = np.squeeze(
                        np.c_[train_bodies_vec, cosine_vec, train_stances_vec])
                    self.train_features.append(feature_vec)
                    self.train_stances.append(stance_encoder.transform([stance]).item())
            torch.save(self.train_features, "pickled_training_2b_tfidf_features")
            torch.save(self.train_stances, "pickled_training_2b_tfidf_stances")
        self.class_weights_2b = class_weight.compute_class_weight('balanced',
                                                np.unique(self.train_stances),
                                                self.train_stances)


    def __len__(self):
        return len(self.train_features)

    def get_vocab_size(self):
        return self.count_vectorizer.get_feature_names()

    def get_feature_size(self):
        return len(self.train_features[0])

    def __getitem__(self, i):
        return self.train_features[i], self.train_stances[i]

import pandas as pd
import torch
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import os
import pickle
from spacy.lang.en.stop_words import STOP_WORDS
from scipy.spatial.distance import cosine
from torch.utils.data import Dataset
from sklearn.preprocessing import LabelEncoder
pd.set_option('display.max_columns', None)

import pandas as pd
import torch
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import os
import pickle
from spacy.lang.en.stop_words import STOP_WORDS
from scipy.spatial.distance import cosine
from torch.utils.data import Dataset
from sklearn.preprocessing import LabelEncoder
pd.set_option('display.max_columns', None)


def convert_into_related(label):
    if label == "agree" or label == "disagree" or label == "discuss":
        label = "related"
    return label


class TestingTFIDFDataset(Dataset):
    def __init__(self):
        related_encoder = LabelEncoder()
        related_encoder.fit(["related", "unrelated"])
        stance_encoder = LabelEncoder()
        stance_encoder.fit(["agree", "disagree", "discuss", "unrelated"]) # we add unrelated for this one too

        # Load datasets
        train_bodies = pd.read_csv("train_bodies.csv")

        train_stances = pd.read_csv("train_stances.csv")

        test_bodies = pd.read_csv("competition_test_bodies.csv")

        test_stances = pd.read_csv("competition_test_stances.csv")

        # Merge the two into one
        train_merge = pd.merge(train_bodies, train_stances,
                            on="Body ID", how="left")
        test_merge = pd.merge(test_bodies, test_stances,
                            on="Body ID", how="left")

        self.train_bodies = []
        self.train_headlines = []
        self.test_bodies = []
        self.test_headlines = []
        self.test_features = []
        self.test_stances = []
        self.test_relateds = []

        for row in train_merge.iterrows():
            body = row[1]["articleBody"]
            headline = row[1]["Headline"]
            if body not in self.train_bodies:
                self.train_bodies.append(body)
            if headline not in self.train_headlines:
                self.train_headlines.append(headline)

        for row in test_merge.iterrows():
            body = row[1]["articleBody"]
            headline = row[1]["Headline"]
            if body not in self.test_bodies:
                self.test_bodies.append(body)
            if headline not in self.test_headlines:
                self.test_headlines.append(headline)

        # Run vectorizer on them
        if os.path.isfile("pickled_tfidf_vectorizer"):
            self.tfidf_vectorizer = pickle.load(open("pickled_tfidf_vectorizer", "rb"))
        else:
            self.tfidf_vectorizer = TfidfVectorizer(
                lowercase=True, stop_words=STOP_WORDS, max_features=512)
            self.tfidf_vectorizer = self.tfidf_vectorizer.fit(self.train_headlines + self.train_bodies + self.test_headlines + self.test_bodies)
            pickle.dump(self.tfidf_vectorizer, open("pickled_tfidf_vectorizer", "wb"))

        if os.path.isfile("pickled_count_vectorizer"):
            self.count_vectorizer = pickle.load(open("pickled_count_vectorizer", "rb"))
        else:
            self.count_vectorizer = CountVectorizer(
                lowercase=True, stop_words=STOP_WORDS, max_features=512)
            self.count_vectorizer = self.count_vectorizer.fit(self.train_headlines + self.train_bodies + self.test_headlines + self.test_bodies)
            pickle.dump(self.count_vectorizer, open("pickled_count_vectorizer", "wb"))
        
        if os.path.isfile("pickled_testing_tfidf_features") and os.path.isfile("pickled_testing_tfidf_stances"):
            self.test_features = torch.load("pickled_testing_tfidf_features")
            self.test_stances = torch.load("pickled_testing_tfidf_stances")
            self.test_relateds = torch.load("pickled_testing_tfidf_relateds")
        else:
            for row in test_merge.sample(1000).iterrows():
                body = row[1]["articleBody"]
                headline = row[1]["Headline"]
                stance = row[1]["Stance"]

                test_bodies_vec = self.tfidf_vectorizer.transform([body]).toarray()
                test_stances_vec = self.tfidf_vectorizer.transform([headline]).toarray()

                # Compute cosine similarity
                cosine_vec = cosine_similarity(test_bodies_vec, test_stances_vec)

                feature_vec = np.squeeze(
                    np.c_[test_bodies_vec, cosine_vec, test_stances_vec])
                self.test_features.append(feature_vec)
                self.test_stances.append(stance_encoder.transform([stance]).item())
                self.test_relateds.append(related_encoder.transform(
                    [convert_into_related(stance)]).item())
            torch.save(self.test_features, "pickled_testing_tfidf_features")
            torch.save(self.test_stances, "pickled_testing_tfidf_stances")
            torch.save(self.test_relateds, "pickled_testing_tfidf_relateds")


    def __len__(self):
        return len(self.test_features)

    def get_vocab_size(self):
        return self.count_vectorizer.get_feature_names()

    def get_feature_size(self):
        return len(self.test_features[0])

    def __getitem__(self, i):
        return self.test_features[i], self.test_stances[i], self.test_relateds[i]


class Testing2bTFIDFDataset(Dataset):
    def __init__(self):
        stance_encoder = LabelEncoder()
        stance_encoder.fit(["agree", "disagree", "discuss"]) # we add unrelated for this one too

        # Load datasets
        train_bodies = pd.read_csv("train_bodies.csv")

        train_stances = pd.read_csv("train_stances.csv")

        test_bodies = pd.read_csv("competition_test_bodies.csv")

        test_stances = pd.read_csv("competition_test_stances.csv")

        # Merge the two into one
        train_merge = pd.merge(train_bodies, train_stances,
                            on="Body ID", how="left")
        test_merge = pd.merge(test_bodies, test_stances,
                            on="Body ID", how="left")

        self.train_bodies = []
        self.train_headlines = []
        self.test_bodies = []
        self.test_headlines = []
        self.test_features = []
        self.test_stances = []

        for row in train_merge.iterrows():
            body = row[1]["articleBody"]
            headline = row[1]["Headline"]
            if body not in self.train_bodies:
                self.train_bodies.append(body)
            if headline not in self.train_headlines:
                self.train_headlines.append(headline)

        for row in test_merge.iterrows():
            body = row[1]["articleBody"]
            headline = row[1]["Headline"]
            if body not in self.test_bodies:
                self.test_bodies.append(body)
            if headline not in self.test_headlines:
                self.test_headlines.append(headline)

        # Run vectorizer on them
        if os.path.isfile("pickled_tfidf_vectorizer"):
            self.tfidf_vectorizer = pickle.load(open("pickled_tfidf_vectorizer", "rb"))
        else:
            self.tfidf_vectorizer = TfidfVectorizer(
                lowercase=True, stop_words=STOP_WORDS, max_features=512)
            self.tfidf_vectorizer = self.tfidf_vectorizer.fit(self.train_headlines + self.train_bodies + self.test_headlines + self.test_bodies)
            pickle.dump(self.tfidf_vectorizer, open("pickled_tfidf_vectorizer", "wb"))

        if os.path.isfile("pickled_count_vectorizer"):
            self.count_vectorizer = pickle.load(open("pickled_count_vectorizer", "rb"))
        else:
            self.count_vectorizer = CountVectorizer(
                lowercase=True, stop_words=STOP_WORDS, max_features=512)
            self.count_vectorizer = self.count_vectorizer.fit(self.train_headlines + self.train_bodies + self.test_headlines + self.test_bodies)
            pickle.dump(self.count_vectorizer, open("pickled_count_vectorizer", "wb"))
        
        if os.path.isfile("pickled_testing_2b_tfidf_features") and os.path.isfile("pickled_testing_2b_tfidf_stances"):
            self.test_features = torch.load("pickled_testing_2b_tfidf_features")
            self.test_stances = torch.load("pickled_testing_2b_tfidf_stances")
        else:
            for row in test_merge.sample(1000).iterrows():
                body = row[1]["articleBody"]
                headline = row[1]["Headline"]
                stance = row[1]["Stance"]
                if stance != "unrelated":
                    test_bodies_vec = self.tfidf_vectorizer.transform([body]).toarray()
                    test_stances_vec = self.tfidf_vectorizer.transform([headline]).toarray()

                    # Compute cosine similarity
                    cosine_vec = cosine_similarity(test_bodies_vec, test_stances_vec)

                    feature_vec = np.squeeze(
                        np.c_[test_bodies_vec, cosine_vec, test_stances_vec])
                    self.test_features.append(feature_vec)
                    self.test_stances.append(stance_encoder.transform([stance]).item())
            torch.save(self.test_features, "pickled_testing_2b_tfidf_features")
            torch.save(self.test_stances, "pickled_testing_2b_tfidf_stances")


    def __len__(self):
        return len(self.test_features)

    def get_vocab_size(self):
        return self.count_vectorizer.get_feature_names()

    def get_feature_size(self):
        return len(self.test_features[0])

    def __getitem__(self, i):
        return self.test_features[i], self.test_stances[i]

"""## Define BERT Datasets"""

#! pip install transformers
from transformers import BertTokenizer, BertModel
import pandas as pd
import torch
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import os
import pickle
from spacy.lang.en.stop_words import STOP_WORDS
from scipy.spatial.distance import cosine
from torch.utils.data import Dataset
from sklearn.preprocessing import LabelEncoder
pd.set_option('display.max_columns', None)


def convert_into_related(label):
    if label == "agree" or label == "disagree" or label == "discuss":
        label = "related"
    return label


class TrainingBERTDataset(Dataset):
    def __init__(self):
        related_encoder = LabelEncoder()
        related_encoder.fit(["related", "unrelated"])
        stance_encoder = LabelEncoder()
        stance_encoder.fit(["agree", "disagree", "discuss", "unrelated"]) # we add unrelated for this one too

        # Load datasets
        train_bodies = pd.read_csv("train_bodies.csv")

        train_stances = pd.read_csv("train_stances.csv")

        test_bodies = pd.read_csv("competition_test_bodies.csv")

        test_stances = pd.read_csv("competition_test_stances.csv")

        # Merge the two into one
        train_merge = pd.merge(train_bodies, train_stances,
                            on="Body ID", how="left")
        test_merge = pd.merge(test_bodies, test_stances,
                            on="Body ID", how="left")

        self.train_bodies = []
        self.train_headlines = []
        self.test_bodies = []
        self.test_headlines = []
        self.train_features = []
        self.train_stances = []
        self.train_relateds = []

        for row in train_merge.iterrows():
            body = row[1]["articleBody"]
            headline = row[1]["Headline"]
            if body not in self.train_bodies:
                self.train_bodies.append(body)
            if headline not in self.train_headlines:
                self.train_headlines.append(headline)

        for row in test_merge.iterrows():
            body = row[1]["articleBody"]
            headline = row[1]["Headline"]
            if body not in self.test_bodies:
                self.test_bodies.append(body)
            if headline not in self.test_headlines:
                self.test_headlines.append(headline)


        self.model = BertModel.from_pretrained("bert-base-uncased")
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

        if os.path.isfile("pickled_count_vectorizer"):
            self.count_vectorizer = pickle.load(open("pickled_count_vectorizer", "rb"))
        else:
            self.count_vectorizer = CountVectorizer(
                lowercase=True, stop_words=STOP_WORDS, max_features=512)
            self.count_vectorizer = self.count_vectorizer.fit(self.train_headlines + self.train_bodies + self.test_headlines + self.test_bodies)
            pickle.dump(self.count_vectorizer, open("pickled_count_vectorizer", "wb"))
        
        if os.path.isfile("pickled_training_bert_features") and os.path.isfile("pickled_training_bert_stances" and os.path.isfile("pickled_training_bert_relateds")):
            self.train_features = torch.load("pickled_training_bert_features")
            self.train_stances = torch.load("pickled_training_bert_stances")
            self.train_relateds = torch.load("pickled_training_bert_relateds")
        else:
            for row in train_merge.sample(1000).iterrows():
                body = row[1]["articleBody"]
                headline = row[1]["Headline"]
                stance = row[1]["Stance"]

                encoded_body = self.tokenizer(body, return_tensors='pt', max_length=512, truncation=True)
                train_bodies_vec = self.model(**encoded_body).last_hidden_state[:,-1,:].detach().numpy()

                encoded_headline = self.tokenizer(headline, return_tensors='pt', max_length=512, truncation=True)
                train_stances_vec = self.model(**encoded_headline).last_hidden_state[:,-1,:].detach().numpy()

                # Compute cosine similarity
                cosine_vec = cosine_similarity(train_bodies_vec, train_stances_vec)

                feature_vec = np.squeeze(
                    np.c_[train_bodies_vec, cosine_vec, train_stances_vec])
                self.train_features.append(feature_vec)
                self.train_stances.append(stance_encoder.transform([stance]).item())
                self.train_relateds.append(related_encoder.transform(
                    [convert_into_related(stance)]).item())
            torch.save(self.train_features, "pickled_training_bert_features")
            torch.save(self.train_stances, "pickled_training_bert_stances")
            torch.save(self.train_relateds, "pickled_training_bert_relateds")
        self.class_weights_2b = class_weight.compute_class_weight('balanced',
                                                np.unique(self.train_stances),
                                                self.train_stances)
        self.class_weights_2a = class_weight.compute_class_weight('balanced',
                                                np.unique(self.train_relateds),
                                                self.train_relateds)


    def __len__(self):
        return len(self.train_features)

    def get_vocab_size(self):
        return self.count_vectorizer.get_feature_names()

    def get_feature_size(self):
        return len(self.train_features[0])

    def __getitem__(self, i):
        return self.train_features[i], self.train_stances[i], self.train_relateds[i]

import pandas as pd
import torch
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import os
import pickle
from spacy.lang.en.stop_words import STOP_WORDS
from scipy.spatial.distance import cosine
from torch.utils.data import Dataset
from sklearn.preprocessing import LabelEncoder
from transformers import BertTokenizer, BertModel
pd.set_option('display.max_columns', None)

from transformers import BertTokenizer, BertModel
import pandas as pd
import torch
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
import os
import pickle
from spacy.lang.en.stop_words import STOP_WORDS
from scipy.spatial.distance import cosine
from torch.utils.data import Dataset
from sklearn.preprocessing import LabelEncoder
from sklearn.utils import class_weight
pd.set_option('display.max_columns', None)


def convert_into_related(label):
    if label == "agree" or label == "disagree" or label == "discuss":
        label = "related"
    return label


class TestingBERTDataset(Dataset):
    def __init__(self):
        related_encoder = LabelEncoder()
        related_encoder.fit(["related", "unrelated"])
        stance_encoder = LabelEncoder()
        stance_encoder.fit(["agree", "disagree", "discuss", "unrelated"]) # we add unrelated for this one too

        # Load datasets
        train_bodies = pd.read_csv("train_bodies.csv")

        train_stances = pd.read_csv("train_stances.csv")

        test_bodies = pd.read_csv("competition_test_bodies.csv")

        test_stances = pd.read_csv("competition_test_stances.csv")

        # Merge the two into one
        train_merge = pd.merge(train_bodies, train_stances,
                            on="Body ID", how="left")
        test_merge = pd.merge(test_bodies, test_stances,
                            on="Body ID", how="left")

        self.train_bodies = []
        self.train_headlines = []
        self.test_bodies = []
        self.test_headlines = []
        self.test_features = []
        self.test_stances = []
        self.test_relateds = []

        for row in train_merge.iterrows():
            body = row[1]["articleBody"]
            headline = row[1]["Headline"]
            if body not in self.train_bodies:
                self.train_bodies.append(body)
            if headline not in self.train_headlines:
                self.train_headlines.append(headline)

        for row in test_merge.iterrows():
            body = row[1]["articleBody"]
            headline = row[1]["Headline"]
            if body not in self.test_bodies:
                self.test_bodies.append(body)
            if headline not in self.test_headlines:
                self.test_headlines.append(headline)

        

        self.model = BertModel.from_pretrained("bert-base-uncased")
        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')

        if os.path.isfile("pickled_count_vectorizer"):
            self.count_vectorizer = pickle.load(open("pickled_count_vectorizer", "rb"))
        else:
            self.count_vectorizer = CountVectorizer(
                lowercase=True, stop_words=STOP_WORDS, max_features=512)
            self.count_vectorizer = self.count_vectorizer.fit(self.train_headlines + self.train_bodies + self.test_headlines + self.test_bodies)
            pickle.dump(self.count_vectorizer, open("pickled_count_vectorizer", "wb"))
        
        if os.path.isfile("pickled_testing_bert_features") and os.path.isfile("pickled_testing_bert_stances" and os.path.isfile("pickled_testing_bert_relateds")):
            self.test_features = torch.load("pickled_testing_bert_features")
            self.test_stances = torch.load("pickled_testing_bert_stances")
            self.test_relateds = torch.load("pickled_testing_bert_relateds")
        else:
            for row in test_merge.iterrows():
                body = row[1]["articleBody"]
                headline = row[1]["Headline"]
                stance = row[1]["Stance"]

                encoded_body = self.tokenizer(body, return_tensors='pt', max_length=512, truncation=True)
                test_bodies_vec = self.model(**encoded_body).last_hidden_state[:,-1,:].detach().numpy()

                encoded_headline = self.tokenizer(headline, return_tensors='pt', max_length=512, truncation=True)
                test_stances_vec = self.model(**encoded_headline).last_hidden_state[:,-1,:].detach().numpy()

                # Compute cosine similarity
                cosine_vec = cosine_similarity(test_bodies_vec, test_stances_vec)

                feature_vec = np.squeeze(
                    np.c_[test_bodies_vec, cosine_vec, test_stances_vec])
                self.test_features.append(feature_vec)
                self.test_stances.append(stance_encoder.transform([stance]).item())
                self.test_relateds.append(related_encoder.transform(
                    [convert_into_related(stance)]).item())
            torch.save(self.test_features, "pickled_testing_bert_features")
            torch.save(self.test_stances, "pickled_testing_bert_stances")
            torch.save(self.test_relateds, "pickled_testing_bert_relateds")


    def __len__(self):
        return len(self.test_features)

    def get_vocab_size(self):
        return self.count_vectorizer.get_feature_names()

    def get_feature_size(self):
        return len(self.test_features[0])

    def __getitem__(self, i):
        return self.test_features[i], self.test_stances[i], self.test_relateds[i]

"""## Initialize 2a DL model

"""

from operator import index
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm
import time

class RelatedUnrelatedClassifier(torch.nn.Module):
    def __init__(self, feature_size, vocab_size=500000, embed_dim=50, hidden_size=2, num_class=2, num_layers=4):
        super(RelatedUnrelatedClassifier, self).__init__()
        self.hidden_size = hidden_size
        self.lstm = torch.nn.LSTM(input_size=1, hidden_size=hidden_size,
                                  num_layers=num_layers, dropout=0.2, bidirectional=True, batch_first=True)
        self.dropout_2 = torch.nn.Dropout(p=0.2)
        self.attention = torch.nn.MultiheadAttention(embed_dim=hidden_size, num_heads=1)
        self.dense_1 = torch.nn.Linear(in_features=hidden_size, out_features=hidden_size*2)
        self.relu = torch.nn.ReLU()
        self.dense_2 = torch.nn.Linear(in_features=hidden_size*2, out_features=num_class)
        self.softmax = torch.nn.Softmax(dim=1)

    def forward(self, x):
        x = x.view(x.size(0),-1,1)
        x = x.float()
        x = self.dropout_2(x)
        _, (x, _) = self.lstm(x)
        x = x.view(x.size(1), x.size(0), -1)
        x = x[:,-1,:]
        x = x.view(x.size(0),-1)
        x = self.dense_1(x)
        x = self.relu(x)
        x = self.dense_2(x)
        x = self.softmax(x)
        return x

"""## Run 2a DL model with TFIDF"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

epochs = 150000

train_dataset = TrainingTFIDFDataset()
train_dataloader = torch.utils.data.DataLoader(train_dataset,
                                               shuffle=True,
                                               batch_size=32)

test_dataset = TestingTFIDFDataset()
test_dataloader = torch.utils.data.DataLoader(test_dataset,
                                               shuffle=True,
                                               batch_size=32)
test_iter = iter(test_dataloader)
vocab_size = len(train_dataset.get_vocab_size())
feature_size = train_dataset.get_feature_size()
embed_dim = 128
hidden_size = 400
num_class = 2  # related/unrelated
model_2a_tfidf = RelatedUnrelatedClassifier(feature_size,
    vocab_size=vocab_size, embed_dim=embed_dim, hidden_size=hidden_size, num_class=num_class).to(device)
print(train_dataset.class_weights_2a)
criterion = torch.nn.CrossEntropyLoss(weight=torch.as_tensor(train_dataset.class_weights_2a).float().to(device))
optimizer = torch.optim.Adam(model_2a_tfidf.parameters(), lr=1e-4)

test_interval=1000
epoch = 50

if os.path.isfile('2a_tfidf_dl_model.pth'):
    print("2a tfidf model loading!")
    model_2a_tfidf = torch.load('2a_tfidf_dl_model.pth')
    print("2a tfidf model loaded!")
else:
    # enumerate over merge, reading labels
    for i in range(2):
        total_loss = 0
        total_count = 0
        for i, (feature, stance, related) in tqdm(enumerate(train_dataloader)):
            feature = feature.to(device)
            related = related.to(device)
            predicted_label = model_2a_tfidf(feature)
            loss = criterion(predicted_label, related)
            total_loss += loss
            total_count += feature.size(0)
            print(predicted_label)
            print(loss)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
        print("average loss")
        print(total_loss / total_count)

#assert False

test_dataloader = torch.utils.data.DataLoader(test_dataset,
                                               shuffle=True,
                                               batch_size=32)
test_iter = iter(test_dataloader)
with torch.no_grad():
    model_2a_tfidf.eval()
    total_acc = 0
    total_count = 0
    for test_feature, test_stance, test_related in tqdm(test_iter):
        test_feature = test_feature.to(device)
        test_stance = test_stance.to(device)
        test_related = test_related.to(device)
        predicted_test_label = model_2a_tfidf(test_feature)
        print(predicted_test_label)
        total_acc += (predicted_test_label.argmax(1) == test_related).sum().item()
        total_count += test_related.size(0)
    print('| accuracy {:8.3f}'.format(
        total_acc/total_count))
    torch.save(model_2a_tfidf, '2a_tfidf_dl_model.pth')
    model_2a_tfidf.train()

"""## Run 2a DL model with BERT"""

#
#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

epochs = 150000

train_dataset = TrainingBERTDataset()
train_dataloader = torch.utils.data.DataLoader(train_dataset,
                                               shuffle=True,
                                               batch_size=32)

test_dataset = TestingBERTDataset()
test_dataloader = torch.utils.data.DataLoader(test_dataset,
                                               shuffle=True,
                                               batch_size=32)
test_iter = iter(test_dataloader)

vocab_size = len(train_dataset.get_vocab_size())
feature_size = train_dataset.get_feature_size()
num_class = 2  # related/unrelated
model_2a_bert = RelatedUnrelatedClassifier(feature_size,
    vocab_size=vocab_size, embed_dim=embed_dim, hidden_size=hidden_size, num_class=num_class).to(device)
criterion = torch.nn.CrossEntropyLoss(weight=torch.as_tensor(train_dataset.class_weights_2a).float().to(device))
optimizer = torch.optim.Adam(model_2a_bert.parameters(), lr=1e-4)

test_interval=1000


if os.path.isfile('2a_bert_dl_model.pth'):
    print("2a bert model loading!")
    model_2a_bert = torch.load('2a_bert_dl_model.pth')
    print("2a bert model loaded!")
else:
    # enumerate over merge, reading labels
    for i, (feature, stance, related) in tqdm(enumerate(train_dataloader)):
        feature = feature.to(device)
        related = related.to(device)
        optimizer.zero_grad()
        predicted_label = model_2a_bert(feature)
        loss = criterion(predicted_label, related)
        loss.backward()
        optimizer.step()


test_dataloader = torch.utils.data.DataLoader(test_dataset,
                                               shuffle=True,
                                               batch_size=32)
test_iter = iter(test_dataloader)

with torch.no_grad():
    model_2a_bert.eval()
    total_acc = 0
    total_count = 0
    for test_feature, test_stance, test_related in tqdm(test_iter):
        test_feature = test_feature.to(device)
        test_stance = test_stance.to(device)
        test_related = test_related.to(device)
        predicted_test_label = model_2a_bert(test_feature)
        print(predicted_test_label)
        total_acc += (predicted_test_label.argmax(1) == test_related).sum().item()
        total_count += test_related.size(0)
    print('| accuracy {:8.3f}'.format(
        total_acc/total_count))
    torch.save(model_2a_bert, '2a_bert_dl_model.pth')
    model_2a_bert.train()

"""## Initialize and run TFIDF 2a ML model (Logistic Regression)"""

from operator import index
import torch
from sklearn.linear_model import LogisticRegression
from torch.utils.data import DataLoader
from tqdm import tqdm
import time
import numpy as np

#
#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

epochs = 150000

model_lr = LogisticRegression()

train_dataset = TrainingTFIDFDataset()
train_dataloader = torch.utils.data.DataLoader(train_dataset,
                                               shuffle=True,
                                               batch_size=len(train_dataset))
train_iter = iter(train_dataloader)
entire_train = next(train_iter)

test_dataset = TestingTFIDFDataset()
test_dataloader = torch.utils.data.DataLoader(test_dataset,
                                               shuffle=True,
                                               batch_size=len(test_dataset))
test_iter = iter(test_dataloader)
entire_test = next(test_iter)

vocab_size = len(train_dataset.get_vocab_size())


model_lr.fit(entire_train[0], entire_train[2])

y_pred = model_lr.predict(entire_test[0])
accuracy = np.sum(y_pred == list(entire_test[2])) / len(test_dataset)
print("Logistic Regression Accuracy:")
print(accuracy)


# 2b

## Run 2b DL model with BERT

#
#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

epochs = 150000

train_dataset = Training2bTFIDFDataset()
train_dataloader = torch.utils.data.DataLoader(train_dataset,
                                               shuffle=True,
                                               batch_size=32)



test_dataset = Testing2bTFIDFDataset()
test_dataloader = torch.utils.data.DataLoader(test_dataset,
                                               shuffle=True,
                                               batch_size=32)
test_iter = iter(test_dataloader)
vocab_size = len(train_dataset.get_vocab_size())
feature_size = train_dataset.get_feature_size()
num_class = 3  # related/unrelated
model_2b = RelatedUnrelatedClassifier(feature_size,
    vocab_size=vocab_size, embed_dim=embed_dim, hidden_size=hidden_size, num_class=num_class).to(device)
criterion = torch.nn.CrossEntropyLoss(weight=torch.as_tensor(train_dataset.class_weights_2b).float().to(device))
optimizer = torch.optim.Adam(model_2b.parameters(), lr=1e-4)

test_interval=1000

if os.path.isfile('model_2b.pth'):
    print("2b model loading!")
    model_2b = torch.load('model_2b.pth')
    print("2b model loaded!")
else:
    # enumerate over merge, reading labels
    for i, (feature, stance) in tqdm(enumerate(train_dataloader)):
        feature = feature.to(device)
        stance = stance.to(device)
        optimizer.zero_grad()
        predicted_label = model_2b(feature)
        print(stance)
        print(predicted_label)
        loss = criterion(predicted_label, stance)
        loss.backward()
        optimizer.step()


test_dataloader = torch.utils.data.DataLoader(test_dataset,
                                               shuffle=True,
                                               batch_size=32)
test_iter = iter(test_dataloader)
with torch.no_grad():
    model_2b.eval()
    total_acc = 0
    total_count = 0
    for test_feature, test_stance in tqdm(test_iter):
        test_feature = test_feature.to(device)
        test_stance = test_stance.to(device)
        predicted_test_label = model_2b(test_feature)
        total_acc += (predicted_test_label.argmax(1) == test_stance).sum().item()
        total_count += test_stance.size(0)
    print('| accuracy {:8.3f}'.format(
        total_acc/total_count))
    torch.save(model_2b, 'model_2b.pth')
    model_2b.train()

"""## End to end"""

# When I tested this, TFIDF got the best score after regression, so that's what I use for my DL mode
#device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

test_dataset = TestingTFIDFDataset()
test_dataloader = torch.utils.data.DataLoader(test_dataset,
                                               shuffle=True,
                                               batch_size=32)
test_iter = iter(test_dataloader)
vocab_size = len(test_dataset.get_vocab_size())
feature_size = test_dataset.get_feature_size()
num_class = 3  # related/unrelated

test_interval=1000

with torch.no_grad():
    model_2b.eval()
    # enumerate over merge, reading labels
    total_acc = 0
    total_count = 0
    for feature, stance, related in tqdm(test_dataloader):
        feature = feature.to(device)
        stance = related.to(device)
        predicted_label_related = torch.as_tensor(model_lr.predict(feature.cpu())).to(device)
        predicted_label_related = predicted_label_related.view(-1,1)
        predicted_label_stance = model_2b(feature)
        print(predicted_label_stance.argmax(1))
        # where 1s are in the tensor, meaning unrelated, increase to 3
        predicted_label = torch.where(predicted_label_related == 1, predicted_label_related + 3, predicted_label_stance.argmax(1))
        total_acc += (predicted_label.argmax(0) == stance).sum().item()
        total_count += stance.size(0)
    print('| accuracy {:8.3f}'.format(total_acc/total_count))